{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from numpy import genfromtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Model\n",
    "\n",
    "train = genfromtxt('train_solved.csv', delimiter=',')\n",
    "test = genfromtxt('test_solved.csv', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the input data and labels\n",
    "train_data = (train[:,0:784])/255\n",
    "train_labels = train[:,784]\n",
    "\n",
    "test_data = (test[:,0:784])/255\n",
    "test_labels = test[:,784]\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training an autoencoder model on cifar-10 PCA reduced data\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "crrpt_img = Dropout(0.5)(input_img)\n",
    "encoded = Dense(1000, activation='sigmoid')(crrpt_img)\n",
    "decoded = Dense(784, activation='linear')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img,decoded)\n",
    "\n",
    "nb_epoch = 10\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='mean_squared_error',\n",
    "                    metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 92s - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0128 - val_mean_squared_error: 0.0128\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 88s - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0110 - val_mean_squared_error: 0.0110\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 84s - loss: 0.0161 - mean_squared_error: 0.0161 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 90s - loss: 0.0145 - mean_squared_error: 0.0145 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 91s - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 91s - loss: 0.0130 - mean_squared_error: 0.0130 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 111s - loss: 0.0125 - mean_squared_error: 0.0125 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 134s - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 120s - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 108s - loss: 0.0118 - mean_squared_error: 0.0118 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(train_data, train_data,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_data, test_data),\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "autoencoder.save('DAE_l1_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.layers import load_model\n",
    "#autoencoder1 = load_model('DAE_l1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Model(input_img,encoded)\n",
    "htrain_data = encoder.predict(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              1001000   \n",
      "=================================================================\n",
      "Total params: 2,002,000\n",
      "Trainable params: 2,002,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img1 = Input(shape=(1000,))\n",
    "crrpt_img1 = Dropout(0.5)(input_img1)\n",
    "encoded1 = Dense(1000, activation='sigmoid')(crrpt_img1)\n",
    "decoded1 = Dense(1000, activation='sigmoid')(encoded1)\n",
    "\n",
    "autoencoder1 = Model(input_img1,decoded1)\n",
    "\n",
    "autoencoder1.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['binary_crossentropy'])\n",
    "\n",
    "autoencoder1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 145s - loss: 0.1570 - binary_crossentropy: 0.1570     ETA: 16 - ETA: 14s - loss: 0\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 138s - loss: 0.1470 - binary_crossentropy: 0.1470   \n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 149s - loss: 0.1447 - binary_crossentropy: 0.1447   \n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 138s - loss: 0.1433 - binary_crossentropy: 0.1433   \n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 122s - loss: 0.1426 - binary_crossentropy: 0.1426   \n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 107s - loss: 0.1421 - binary_crossentropy: 0.1421   \n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 107s - loss: 0.1418 - binary_crossentropy: 0.1418   \n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 111s - loss: 0.1416 - binary_crossentropy: 0.1416   \n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 123s - loss: 0.1414 - binary_crossentropy: 0.1414   \n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 122s - loss: 0.1413 - binary_crossentropy: 0.1413   \n"
     ]
    }
   ],
   "source": [
    "history = autoencoder1.fit(htrain_data, htrain_data,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "autoencoder1.save('DAE_l2_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.layers import load_model\n",
    "#autoencoder1 = load_model('DAE_l2_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 47s - loss: 0.3992 - acc: 0.8828 - val_loss: 0.1416 - val_acc: 0.9561\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 47s - loss: 0.1814 - acc: 0.9450 - val_loss: 0.1044 - val_acc: 0.9665\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 47s - loss: 0.1420 - acc: 0.9562 - val_loss: 0.0857 - val_acc: 0.9720\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 47s - loss: 0.1207 - acc: 0.9624 - val_loss: 0.0798 - val_acc: 0.9763\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 46s - loss: 0.1060 - acc: 0.9663 - val_loss: 0.0754 - val_acc: 0.9759\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 47s - loss: 0.0905 - acc: 0.9721 - val_loss: 0.0736 - val_acc: 0.9778\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 47s - loss: 0.0824 - acc: 0.9735 - val_loss: 0.0734 - val_acc: 0.9770\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 47s - loss: 0.0745 - acc: 0.9762 - val_loss: 0.0767 - val_acc: 0.9777\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 47s - loss: 0.0707 - acc: 0.9770 - val_loss: 0.0702 - val_acc: 0.9786\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 49s - loss: 0.0683 - acc: 0.9774 - val_loss: 0.0697 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "# Training with unsupervised initialization for layer-1 \n",
    "# of an MLP using autoencoder weights\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(784,)))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(1000, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mlp.layers[1].set_weights(autoencoder.layers[2].get_weights())\n",
    "mlp.layers[3].set_weights(autoencoder1.layers[2].get_weights())\n",
    "\n",
    "\n",
    "history = mlp.fit(train_data[:25000], train_labels[:25000],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
